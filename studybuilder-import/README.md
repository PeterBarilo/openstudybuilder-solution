# Introduction 
This repository contains scripts for populating the Neo4j MDR Database
with the dictionaries and sponsor data that StudyBuilder requires.

It also contains some sample study data.

# Python Getting Started

This section is written for running on Ubuntu (that uses the `apt` package manager).
Other systems can also be used, but the commands for installing packages will be different.

This project uses https://github.com/pypa/pipenv for dependencies management,
so you must install it on your system.
Version 2020.8.13 or later is needed.
Ubuntu 20.04 ships an old version, so use `pip` instead of `apt` to install pipenv.

```
$ python3 --version
$ pip --version
$ pip install --upgrade pip pipenv
```

**Note:** In case the required Python version is not available on your system,
it is most likely available from the _deadsnakes_ PPA.
To enable the PPA and install the needed Python version with:

```
$ sudo add-apt-repository ppa:deadsnakes/ppa
$ sudo apt-get update
$ sudo apt-get install python 3.11-dev
```


# Running
## Preparations
This

These migration scripts populate the database via the api (`clinical-mdr-api`).
Thus they require both the Neo4j database and the clinical-mdr-api to be running.
They also require the clinical standards data to be available in the database.

See the READMEs in the `neo4j-mdr-db`, `mdr-standards-import` and `clinical-mdr-api`
repositories on how to start and initialize the database, import standards, and start the api.

Once the preparations are done, continue with setting up an environment file, `.env`.

## Setup environment variables
Create `.env` file (in the root of cloned repository). Use the `.env.import` file as a template and adjust as needed.

The most important variable to configure is the url of the api:
```
API_BASE_URL="http://localhost:8000"
```
Adjust this if the api is not running on localhost at port 8000.

**About** ***API_BASE_URL***

As a default the Clinical MDR API is accessible through a specific base URL when it is running. 
The base URL on a local environment with default configurations is usually http://localhost:8000. 
The base URL is the core of all URLs generated by various endpoints in the API. 

***Exmaple:** If the base url is http://localhost:8000 and a user wishes to access the endpoint "/criteria/{uid}/studies",
the full URL would be http://localhost:8000/criteria/{uid}/studies.*

The variable `API_BASE_URL` defined in the `.env` file, is used by the data import component to import data through the endpoints.
Therefore it must be equal to the the base URL of the API instance that is to be used for this data import. 
It is possible to configure the API to use a different port.
In that case, the port number must be updated in API_BASE_URL of the `.env` file of the data import component.

There are three more general variables:
```
LOG_LEVEL=INFO
MDR_MIGRATION_SAMPLE=False
MDR_MIGRATION_FROM_SAME_ENV=False
```
`LOG_LEVEL` determines the logging level. `INFO` is the recommended setting for normal use.
This shows info level messages, and hides `DEBUG` and `TRACE`.

`MDR_MIGRATION_SAMPLE` determines if a small subset of data should be imported
rather than the full content of the provided files.
This should normally be set to `False` to ensure a complete set of data is imported.
Only enable this to save time while doing work on the import scripts themselves.

`MDR_MIGRATION_FROM_SAME_ENV` can be set to `True` if the data to be imported
was exported from the same environment.
The import script can then use the provided UIDs for library content.
Otherwise, the script will look up the UIDs in the target environment
by name or submission value.
Setting this to `True` can make sure that linked library content is found
even if the name or submission value has been updated,
and the content to be imported is refering to an older version with a different name.


The rest of the .env-file contains various settings for customizing the behavior of the import script.
It also determines which files are used by each import step.
The `.env.import` example is set up to import everything except the dummy development study,
using the files from the `datatfiles` directory. 

## Activate pipenv

Activate pipenv by running:

```
$ pipenv install
```

## Run full import
Run the following command to start the full import.

```sh
$ pipenv run import_all
```
**Note:** This will take some time to run.

## Partial import

It is also possible to run only a single part of the import with ```pipenv run {scriptname}```. 
The available commands are listed under `[scripts]` in the `Pipfile`.

The current set of commands is:
- import_all (runs all of the others in the order listed here)
- dictionaries
- config
- codelistterms1
- codelistterms2
- unitdefinitions
- activities
- codelistfinish
- compounds
- crfs
- mockdata
- mockdatajson
- sponsormodels


## Running with docker

Actually running Dockerized is the recommended way.

### Setup

On Linux add the `UID` environment variable to the `.env` file in the root 
of the repository folder. The value should correspond to your user's uid. 
You can get that by running the `uid` command. Example `.env` file:

```shell
UID=1001
```

After changing the value of `UID` you need to rebuild the Docker image, 
ignoring the cache:

```shell
docker compose build --no-cache
```

If you run into file permission problems, first check the above steps on any 
architecture.

Check if `API_BASE_URL` is correct in `.env.import` file.

### Run import

Import-all can be run via:

```shell
docker compose up import
```

Any other commands can be run via:

```shell
docker compose run --rm pipenv run whatever
```

# Import steps
This lists the various steps that the import goes through,
as well as what environment variables that determine which files are used.

## Steps
### Dictionaries: `dictionaries`
| step | environment variable(s) |
|------|-------------------------|
| dictionary definitions | MDR_MIGRATION_DICTIONARIES_CODELISTS_DEFINITIONS |
| snomed | MDR_MIGRATION_SNOMED |
| med rt | MDR_MIGRATION_MED_RT |
| unii | MDR_MIGRATION_UNII |
| ucum | MDR_MIGRATION_UCUM |


### Study fields configuration: `config`
| step | environment variable(s) |
|------|-------------------------|
| study field config | MDR_STUDY_FIELDS_DEFINITIONS |

### Additions to codelists
| step | environment variable(s) |
|------|-------------------------|
| Additional terms for the frequency codelist, C71113 | MDR_MIGRATION_FREQUENCY |

### Sponsor library content, standard codelists
| step | environment variable(s) |
|------|-------------------------|
| sponsor codelist definitions. Used to create the lists, no content here. | MDR_MIGRATION_SPONSOR_CODELIST_DEFINITIONS |
| operator (equal, larger than, etc) | MDR_MIGRATION_OPERATOR |
| epoch (collect terms from cdisc in sponsor codelist, add a few terms not in cdisc) | MDR_MIGRATION_EPOCH <br> MDR_MIGRATION_EPOCH_TYPE <br> MDR_MIGRATION_EPOCH_SUB_TYPE |
| endpoint | MDR_MIGRATION_ENDPOINT_LEVEL |
| endpoint (sub) category | MDR_MIGRATION_ENDPOINT_CATEGORY <br> MDR_MIGRATION_ENDPOINT_SUB_CATEGORY |
| activities | MDR_MIGRATION_ACTIVITY_INSTANCES |
| objective | MDR_MIGRATION_OBJECTIVE_LEVEL |
| codelist parameter set, used to mark codelist terms as template parameters | MDR_MIGRATION_CODELIST_PARAMETER_SET |
| arm type | MDR_MIGRATION_ARM_TYPE |
| criteria type | MDR_MIGRATION_CRITERIA_CATEGORY <br> MDR_MIGRATION_CRITERIA_SUB_CATEGORY <br> MDR_MIGRATION_CRITERIA_TYPE |
| compound dispensed in | MDR_MIGRATION_COMPOUND_DISPENSED_IN |
| device | MDR_MIGRATION_DEVICE |
| element (sub)type | MDR_MIGRATION_ELEMENT_TYPE <br> MDR_MIGRATION_ELEMENT_SUBTYPE |
| flowchart group | MDR_MIGRATION_FLOWCHART_GROUP |
| objective category | MDR_MIGRATION_OBJECTIVE_CATEGORY |
| therapy area | MDR_MIGRATION_THERAPY_AREA |
| time reference | MDR_MIGRATION_TIME_REFERENCE |
| type of treatment | MDR_MIGRATION_TYPE_OF_TREATMENT |
| sponsor defined units (extending SDTM) | MDR_MIGRATION_SPONSOR_UNITS |
| unit definitions | MDR_MIGRATION_UNIT_DIF |
| unit dimension | MDR_MIGRATION_UNIT_DIMENSION |
| unit subset | MDR_MIGRATION_UNIT_SUBSETS |
| visit contact mode | MDR_MIGRATION_VISIT_CONTACT_MODE |
| visit type | MDR_MIGRATION_VISIT_TYPE |
| visit sub label | MDR_MIGRATION_VISIT_SUB_LABEL |
| null flavor | MDR_MIGRATION_NULL_FLAVOR |
| sponsor domain | MDR_MIGRATION_SPONSOR_DOMAIN |

### CRFs
| step | environment variable(s) |
|------|-------------------------|
| crf templates | MDR_MIGRATION_ODM_TEMPLATES |
| forms | MDR_MIGRATION_ODM_FORMS |
| groups | MDR_MIGRATION_ODM_ITEMGROUPS |
| items | MDR_MIGRATION_ODM_ITEMS |

### Sponsor terms
| step | environment variable(s) |
|------|-------------------------|
| compounds | MDR_MIGRATION_COMPOUNDS |

### Mock data: `mockdata`
Set up some mock data to have something to show.

| step | environment variable(s) |
|------|-------------------------|
| studies | MDR_MIGRATION_STUDY <br> MDR_MIGRATION_STUDY_TYPE |
| projects | MDR_MIGRATION_PROJECTS |
| objectives | MDR_MOCKUP_OBJECTIVES_OBJECTS |
| endpoints | MDR_MOCKUP_ENDPOINTS_OBJECTS |
| timeframes | MDR_MOCKUP_TIMEFRAMES_OBJECTS |
| study objectives | MDR_MOCKUP_STUDY_OBJECTIVES |
| study endpoints | MDR_MOCKUP_STUDY_ENDPOINTS |

### Exported data from json: `mockdatajson`
(TODO rename the script! It's not only used for mockup data)
This step imports data that is exported by the [studybuilder-export](https://dev.azure.com/orgremoved/Clinical-MDR/_git/studybuilder-export) script.
What files to use is configured by a single variable that points to the exported project listing.
Other files are found via their names that match their corresponding api endpoints. 

```
IMPORT_PROJECTS="datafiles/mockup/exported/projects.json"
```

#### Selective import

There are a number of options for limiting what is imported from json. 
The different parts can be skipped by setting the corresponding variable to `False`.
When importing studies, there are two variables for selecting which studies are imported. 
- `INCLUDE_STUDY_NUMBERS`:  list of study numbers that should be included
- `EXCLUDE_STUDY_NUMBERS`:  study numbers to exclude

The logic is that it first filters the available studies to include 
only the ones in `INCLUDE_STUDY_NUMBERS`.
If `INCLUDE_STUDY_NUMBERS` is not specified, then all are included.

After this it then removes any study listed in `EXCLUDE_STUDY_NUMBERS`.

Both accept a comma-separated list of study numbers, like `1001,1002,1234`.

The standard datafiles includes a dummy study intended to help starting up
development environments.
This is enabled by setting `MDR_MIGRATION_INCLUDE_DUMMY_STUDY=True`.

This creates a study with number 9999 that has some dummy data in most parts.

Setting `MDR_MIGRATION_RENUMBER_DUMMY_STUDY=True` will import the dummy study
as a new study number instead of updating the existing one.
It then starts from 9999 and decrements the number until it finds one that is free.

It's possible to skip parts of the json import.
This is controlled via a set of environment variables that take `True` to enable the import, and `False` for disabling.

| part | variable |
|------|----------|
| clinical programmes | MDR_MIGRATION_EXPORTED_PROGRAMMES |
| brands | MDR_MIGRATION_EXPORTED_BRANDS |
| activities | MDR_MIGRATION_EXPORTED_ACTIVITIES |
| unit definitions | MDR_MIGRATION_EXPORTED_UNITS |
| compounds | MDR_MIGRATION_EXPORTED_COMPOUNDS |
| syntax templates | MDR_MIGRATION_EXPORTED_TEMPLATES |
| projects | MDR_MIGRATION_EXPORTED_PROJECTS |
| studies | MDR_MIGRATION_EXPORTED_STUDIES |

Example that includes everything:
```
MDR_MIGRATION_EXPORTED_PROGRAMMES=True
MDR_MIGRATION_EXPORTED_BRANDS=True
MDR_MIGRATION_EXPORTED_ACTIVITIES=True
MDR_MIGRATION_EXPORTED_UNITS=True
MDR_MIGRATION_EXPORTED_COMPOUNDS=True
MDR_MIGRATION_EXPORTED_TEMPLATES=True
MDR_MIGRATION_EXPORTED_PROJECTS=True
MDR_MIGRATION_EXPORTED_STUDIES=True
INCLUDE_STUDY_NUMBERS=""
EXCLUDE_STUDY_NUMBERS=""
MDR_MIGRATION_INCLUDE_DUMMY_STUDY=True
MDR_MIGRATION_RENUMBER_DUMMY_STUDY=True
```

#  Authentication

## OAuth 2

Supports [OAuth 2.0 client credentials flow with shared secret](https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret).
Credentials can be configured by setting all the following environment variables.
If *CLIENT_ID* is set, the authentication routine is activated.

```shell
CLIENT_ID="aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee"
CLIENT_SECRET="...FILL-ME..."
TOKEN_ENDPOINT="https://login.microsoftonline.com/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/oauth2/v2.0/token"
SCOPE="api://aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee/.default"
```

- **TOKEN_ENDPOINT** is the endpoint where to post the authentication request.
  Can be found in the OpenID Connect metadata document, or Azure Active Directory -> App registrations -> Endpoints.
- **SCOPE** is the scope to request at the authentication flow, and in case of the Microsoft Identity Platform,
  that is the application ID (in URI format) of the API and *.default*
  The main point here is that the OAuth authority should give back a valid access token.
- **CLIENT_ID** is the application id registered for this client application
- **CLIENT_SECRET** is one of the secret key values set up with the client application at the authority

Authentication is done once per migration script session, fetching an access token which is then included in each
request as the *Authorization* header.

## Manually provided token

Instead of using the OAuth 2 flow, it is possible to provide a Bearer token to be used.
This method is used if a token is given in the `STUDYBUILDER_API_TOKEN` environment variable.
```shell
STUDYBUILDER_API_TOKEN=ABCDEFGH123456... (typically 1000+ characters) 
```

The environment variables for the OAuth 2 flow are ignored if `STUDYBUILDER_API_TOKEN` is provided.

# Sample data and scripts

There is a selection of sample Cypher scripts that are useful for testing
and learning more about Cypher and the datamodel. See the `sample_scripts` folder.
These scripts rely on some files in the `datafiles/libraries/concepts/crfs` and `migration_test_data\unused` folders.

# Adding sponsor terms and codelists

## Add a term to an existing list

To add a new term to a sponsor codelist, simply edit the corresponding .csv file.

Let's add a new to the "Compound dispensed in" codelist (with codelist submission value "COMP_DISP_IN").

The file is stored at `./datafiles/sponsor_library/compound_dispensed_in.csv`


| "CT_CD_LIST_SUBMVAL" | "CT_NAME" | "NAME_SENTENSE_CASE" | "CT_SUBMVAL" |"DEFINITION" | "ORDER" |
| -------------------- | --------- | -------------------- | ------------ | ----------- | ------- |
| "COMP_DISP_IN" | "Cartridge" | "cartridge" | "DISPENSED IN CARTRIDGE" | "Cartridge" | "103" |
| "COMP_DISP_IN" | "Pre-filled pen" | "pre-filled pen" | "DISPENSED IN PREFILLED PEN" | "Pre-filled pen" | "116" |
| "COMP_DISP_IN" | "Blister" | "blister" | "DISPENSED IN BLISTER" | "Blister" | "102" |
| "COMP_DISP_IN" | "Vial" | "vial" | "DISPENSED IN VIAL" | "Vial" | "122" |

Since all terms in this list belong to the same codelist, they all have the codelist  

Let's add the term "Ampoule", with submission value (`CT_SUBMVAL`) "DISPENSED IN AMPOULE".

We want it listed just above "Vial" in the UI, so we set order to 121. 

For definition we use "Small sealed glass or plastic vial"

`CT_NAME` is the term name, while `NAME_SENTENCE_CASE` is a variation of the name that is used in running text.

The line to add at the end of the file is thus:
```
"COMP_DISP_IN","Ampoule","ampoule","DISPENSED IN AMPOULE","Small sealed glass or plastic vial","121"
```

After this, run just the `codelistterms2` Pipenv command:
```sh
$ pipenv run codelistterms2
```

## Add a new codelist

Adding a new codelist consists of a few more steps than adding a single term.
This example will add the dummy codelist "Icecream Flavor", with the terms "Chocolate" and "Vanilla".

### Add the codelist definition

The sponsor codelists are defined in the file `datafiles/sponsor_library/sponsor_codelist_definitions.csv`.

We need to add a line at the end to define the new codelist:

| library | catalogue | legacy_codelist_id | legacy_codelist_name | new_codelist_name | template_parameter | submission_value | preferred_term | definition | extensible | synonyms | href |
| ------- | --------- | ------------------ | -------------------- | ----------------- | ------------------ | ---------------- | -------------- | ---------- | ---------- | -------- | ---- |
| Sponsor | SDTM CT | ICECREAM_FLAVOR | Icecream Flavor | Icecream Flavor | Y | ICECREAM_FLAVOR | Icecream Flavor | "Selection of icecream flavors" | Y | | |

Description of fields
- library: Set to "Sponsor" since this is a sponsor-defined codelist.
- catalogue: The catalogue that this should belong to. SDTM CT is used as an example.
- legacy_codelist_id: If the codelist is migrated from a legacy system, this is the id of the list in that system. Not currently used by the import script.
- legacy_codelist_name: Same as legacy_codelist_id but for name instead of id.
- new_codelist_name: The name of the codelist in StudyBuilder.
- template_parameter: "Y" if the codelist terms should be available as template parameters for use in syntax templates, else "N".
- submission_value: Codelist submission value.
- preferred_term: The name that is displayed.
- definition: The definition, describing the list.
- extensible: "Y" or "N" if the list should be user-extensible.
- synonyms: Synonyms for the codelist. Not currently used by the import script.
- href: Not currently used by the import script.


### Add the codelist terms
Create the new codelist .csv file with the following content:

| CT_CD_LIST_SUBMVAL | CT_NAME   | NAME_SENTENSE_CASE | CT_SUBMVAL       | DEFINITION                        | ORDER | CT_CD  | NCI_PREFERRED_NAME   |
| ------------------ | --------- | ------------------ | ---------------- | --------------------------------- | ----- | ------ | -------------------- |
| ICECREAM_FLAVOR    | Vanilla   | vanilla            | VANILLA FLAVOR   | Plain vanilla flavor              | 1     |        | Vanilla flavor       |
| ICECREAM_FLAVOR    | Chocolate | chocolate          | CHOCOLATE FLAVOR | Flavored with pieces of chocolate | 2     | C99999 | Chocolate flavor     |

Description of fields
- CT_CD_LIST_SUBMVAL: Reference to the Codelist
- CT_NAME: Name of the Term
- NAME_SENTENSE_CASE: Name of the Term but in lowercase
- CT_SUBMVAL: Submission value of the Term (should be in capital)
- DEFINITION: Definition of the Term
- ORDER: Order of the Term in the dedicated Codelist
- CT_CD: Leave empty if it is a newly Term, otherwise, provide the ConceptID of the existing CDISC Term (with Cxxxxxx)
- NCI_PREFERRED_NAME: Provide a valid name that could be used for display purpose

Save this as `./datafiles/sponsor_library/icecream_flavor.csv`

### Add an import step for the new list
Edit the `.env` file to add a new variable for the new file:
(Please add this also to the .env.import and to the .env.e2e for reference)
```
MDR_MIGRATION_ICECREAM_FLAVOR="datafiles/sponsor_library/icecream_flavor.csv"
```

Please remember here to 'duplicate' that evolution inside the folder e2e_datafiles (Otherwise you will get an error in the pipeline)

Edit the script `run_import_standardcodelistterms2.py`.

Find the block of lines near the top where it reads the environment variables,
and add this line at the end likely near line 60:
```
 60| MDR_MIGRATION_ICECREAM_FLAVOR = load_env("MDR_MIGRATION_ICECREAM_FLAVOR")
```

Locate the function `async_run(`)`, which should be around line 147:
```
147|    async def async_run(self):
```

At the end of this function, add a new call to `migrate_term()`, again with approximate line numbers:
```
342|            await self.migrate_term(
343|                MDR_MIGRATION_ICECREAM_FLAVOR,
344|                codelist_name="Icecream Flavor",
345|                code_lists_uids=code_lists_uids,
346|                session=session,
347|            )
```

### Run the import to add the new list and its terms

Start by running the `codelistterms1` Pipenv command:

```sh

$ pipenv run codelistterms1

```

This creates all sponsor codelists, as well as imports the first set of sponsor codelist terms.
It is possible to limit what codelists are imported
by giving a comma-separated list of codelist names.
Enclose the list in quotes, and add no spaces around the comma:
```sh
$ pipenv run codelistterms1 "Epoch,Objectivel Level"
```
This imports only the listed codelists (codelist definitions and terms).

After this, run the `codelistterms2` Pipenv command:
```sh
$ pipenv run codelistterms2
```
This imports all sponsor codelists.
Also here it is possible to limit what codelists are imported
by giving a comma-separated list of codelist names:
```sh
$ pipenv run codelistterms2 "Unit Subset,Footnote Type"
```

## Updating the included demo study data

To extend or add the demo studies, please see the [separate readme](update_demo_data.md).

# Visit Type Excel Converter

## Overview

The `/studybuilder-import/visit_type_excel_converter.py` script is designed to transform data from a Visit Types Excel file into a structured CSV format suitable for loading into StudyBuilder (SB). This script reads the specified Excel file, processes the data according to predefined rules, and generates a new CSV file formatted for SB ingestion.


## Execution

`python visit_type_excel_converter.py <./datafiles/example.xlsx>`

## Sample output

```
Received dir parameter: './datafiles/example.xlsx'
Found the Excel file to convert :)
Filtered Excel data loaded successfully
Found the CSV with example structure :)
CSV data loaded successfully
Result has been written to './output_result.csv'
```

